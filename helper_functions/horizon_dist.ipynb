{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "plt.style.use('default')\n",
    "CSV_EXTENSION = '.csv'\n",
    "SINGLE_FILE_KEY = 'single_file'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def process_horizon_categories_output(row):\n",
    "    # Check if the row is a string\n",
    "    if isinstance(row, str):\n",
    "        # Define the regular expression pattern\n",
    "        pattern = r\"(?P<type>\\w+) '.*?' used at line (?P<usage_line>\\d+) is defined at line (?P<def_line>\\d+)\"\n",
    "\n",
    "        # Use the re.findall function to find all matches in the row\n",
    "        matches = re.findall(pattern, row)\n",
    "\n",
    "        # Convert the matches to the required format\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            type, usage_line, def_line = match\n",
    "            results.append((type, int(usage_line), int(def_line)))\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        # If the row is not a string, return an empty list\n",
    "        return []\n",
    "\n",
    "def expand_lists_to_rows(df, column):\n",
    "    # Create a new DataFrame where each item in the 'Processed' column is expanded into its own row\n",
    "    expanded_df = df.explode(column)\n",
    "\n",
    "    # Split the 'Processed' column into separate 'Type', 'usage_line', and 'def_line' columns\n",
    "    expanded_df[['Type', 'usage_line', 'def_line']] = pd.DataFrame(expanded_df[column].tolist(), index=expanded_df.index)\n",
    "    \n",
    "    # Calculate the absolute differences between 'usage_line' and 'def_line'\n",
    "    expanded_df['abs_diff'] = abs(expanded_df['usage_line'] - expanded_df['def_line'])\n",
    "\n",
    "    # Drop the 'Processed' column\n",
    "    expanded_df = expanded_df.drop(columns=[column])\n",
    "\n",
    "    return expanded_df\n",
    "\n",
    "def process_max_range(expanded_df, groupby_col=['code_task', 'start_line', 'end_line']):\n",
    "    # Drop NaN values\n",
    "    expanded_df = expanded_df.dropna(subset=groupby_col)\n",
    "    # Reset the index\n",
    "    expanded_df = expanded_df.reset_index(drop=True)\n",
    "    # Find the index of the row with the maximum difference for each group\n",
    "    idx = expanded_df.groupby(groupby_col)['abs_diff'].idxmax()\n",
    "    # Print out the number of rows in the original DataFrame, dropped, and the number of rows after the groupby operation\n",
    "    print(f\"Original DataFrame: {len(expanded_df)} rows\")\n",
    "    # print(f\"Rows dropped: {len(expanded_df) - len(idx)}\")\n",
    "    print(f\"DataFrame after groupby: {len(idx)} rows\")\n",
    "    # Keep only the rows with the maximum difference\n",
    "    expanded_df = expanded_df.loc[idx]\n",
    "    return expanded_df\n",
    "\n",
    "def extract_info(csv_file):\n",
    "    # Remove the file extension\n",
    "    name_without_ext = csv_file.split('.csv')[0]\n",
    "\n",
    "    # Split the name into parts\n",
    "    parts = name_without_ext.split('_')\n",
    "\n",
    "    # Extract the information\n",
    "    info = {\n",
    "        'model_name': parts[0],\n",
    "        'gen_mode': '_'.join(parts[1:3]),\n",
    "        'task': parts[3],\n",
    "        'time': '_'.join(parts[4:])\n",
    "    }\n",
    "\n",
    "    return info\n",
    "\n",
    "def group_by_res(folder_path, group_by_keys=['model_name','gen_mode'], group_gpt_4_turbo=True):\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # Create a dictionary to store the groups\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    # Group the CSV files\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the information from the file name\n",
    "        info = extract_info(csv_file)\n",
    "\n",
    "        # Get the group key\n",
    "        group_key_parts = [info[key] for key in group_by_keys]\n",
    "        group_key = '_'.join(group_key_parts)\n",
    "\n",
    "        # If group_gpt_4_turbo is True, treat 'gpt-4-turbo' and 'gpt-4-turbo-2024-04-09' as the same model_name\n",
    "        if group_gpt_4_turbo and 'model_name' in group_by_keys and 'gpt-4-turbo' in group_key:\n",
    "            group_key = group_key.replace('gpt-4-turbo-2024-04-09', 'gpt-4-turbo')\n",
    "\n",
    "        # Add the CSV file to the group\n",
    "        groups[group_key].append(csv_file)\n",
    "\n",
    "    # Concatenate the CSV files in each group\n",
    "    for group_key, group_files in groups.items():\n",
    "        print(f\"Grouped by {group_by_keys} and combined {', '.join(group_files)}\")\n",
    "        dfs = [pd.read_csv(os.path.join(folder_path, csv_file)) for csv_file in group_files]\n",
    "        df_group = pd.concat(dfs, ignore_index=True)\n",
    "        groups[group_key] = df_group\n",
    "\n",
    "    return groups\n",
    "\n",
    "def groupby_sanity_check(grouped_dfs):\n",
    "    for group_key, df in grouped_dfs.items():\n",
    "        print(f\"Group Key: {group_key}\")\n",
    "        print(len(df))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "def process_folder(folder_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    global col_to_check\n",
    "    # Check if the path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Group and concatenate the CSV files\n",
    "        grouped_dfs = group_by_res(folder_path)\n",
    "    elif os.path.isfile(folder_path) and folder_path.endswith(CSV_EXTENSION):\n",
    "        # If the path is a CSV file, read it into a DataFrame\n",
    "        grouped_dfs = {SINGLE_FILE_KEY: pd.read_csv(folder_path)}\n",
    "    else:\n",
    "        # If the path is not a directory and not a CSV file, raise an error\n",
    "        raise ValueError(f\"Invalid path: {folder_path}. Path must be a directory containing CSV files or a CSV file.\")\n",
    "    processed_dfs = {}\n",
    "    for group_key, df in grouped_dfs.items():\n",
    "        init_row_num = len(df)\n",
    "        # Columns to check for NaN values\n",
    "        initial_cols_to_check = ['horizon_categories_output', 'horizon_freq_analysis']\n",
    "\n",
    "        # Drop rows with NaN values in the initial_cols_to_check\n",
    "        df_after_initial_drop = df.dropna(subset=initial_cols_to_check)\n",
    "\n",
    "        # Process the 'horizon_categories_output' column\n",
    "        df_after_initial_drop['Processed'] = df_after_initial_drop['horizon_categories_output'].apply(process_horizon_categories_output)\n",
    "\n",
    "        # Expand the lists to rows\n",
    "        expanded_df = expand_lists_to_rows(df_after_initial_drop, 'Processed')\n",
    "\n",
    "        # Additional columns to check for NaN values\n",
    "        additional_cols_to_check = ['horizon_categories_output', 'horizon_freq_analysis', 'Type', 'usage_line', 'def_line', 'abs_diff', col_to_check]\n",
    "\n",
    "        # Drop rows with NaN values in the additional_cols_to_check\n",
    "        expanded_df_filtered = expanded_df.dropna(subset=additional_cols_to_check)\n",
    "\n",
    "        print(\"#############################################\")\n",
    "        # Print out the number of rows in the original DataFrame, and dropped due to NaN values in the cols_to_check\n",
    "        print(f\"Original DataFrame: {init_row_num} rows\")\n",
    "        print(f\"Rows dropped due to NaN in initial columns {initial_cols_to_check}: {init_row_num - len(df_after_initial_drop)}\")\n",
    "        print(f\"Rows after expansion: {len(expanded_df)} rows\")\n",
    "        print(f\"Rows dropped due to NaN in additional columns {additional_cols_to_check}: {len(expanded_df) - len(expanded_df_filtered)}\")\n",
    "\n",
    "        # Process the max range\n",
    "        df = process_max_range(expanded_df_filtered)\n",
    "\n",
    "        processed_dfs[group_key] = df\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "def convert_to_float(val: str) -> float:\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        parts = val.split('/')\n",
    "        if len(parts) == 2:\n",
    "            num, denom = parts\n",
    "            if denom != '0':\n",
    "                return float(num) / float(denom)\n",
    "        return 0.0\n",
    "    \n",
    "def draw_histogram(df, group_key, show_pass_dist=False, bins=30):\n",
    "    global col_to_check\n",
    "    plt.figure(dpi=400)\n",
    "    # Calculate the absolute differences between 'usage_line' and 'def_line'\n",
    "    differences = df['abs_diff']\n",
    "\n",
    "    # Define the bin edges\n",
    "    bin_edges = np.linspace(differences.min(), differences.max(), bins+1)\n",
    "\n",
    "    # Create a histogram of the differences\n",
    "    plt.hist(differences, bins=bin_edges, edgecolor='black', alpha=0.5, label='All data')\n",
    "\n",
    "    if show_pass_dist:\n",
    "        # Convert 'col_to_check' to float and filter the DataFrame where it is 1\n",
    "        df[col_to_check] = df[col_to_check].apply(convert_to_float)\n",
    "        filtered_df = df[df[col_to_check] == 1]\n",
    "\n",
    "        # Create a histogram of the differences for the filtered DataFrame\n",
    "        plt.hist(filtered_df['abs_diff'], bins=bin_edges, edgecolor='black', color='red', alpha=0.5, label='gen_code_passed')\n",
    "\n",
    "    # Set the title and labels\n",
    "    plt.title(group_key)\n",
    "    plt.xlabel('Recall Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the histogram\n",
    "    plt.show()\n",
    "\n",
    "def plot_histograms(groups, show_pass_dist=True):\n",
    "    global col_to_check\n",
    "    for group_key, df_group in groups.items():\n",
    "        print(f\"Plotting histogram for group: {group_key}\")\n",
    "        draw_histogram(df_group, group_key, show_pass_dist)\n",
    "\n",
    "def classify_and_calculate(df, short_range, medium_range, report_err_bar=True):\n",
    "    global col_to_check\n",
    "    def classify_abs_diff(x):\n",
    "        if x <= short_range:\n",
    "            return 'Short'\n",
    "        elif x <= medium_range:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Long'\n",
    "\n",
    "    df['range_class'] = df['abs_diff'].apply(classify_abs_diff)\n",
    "    df[col_to_check] = df[col_to_check].apply(convert_to_float)\n",
    "\n",
    "    total_counts = df.groupby('range_class').size()\n",
    "    pass_counts = df[df[col_to_check] == 1].groupby('range_class').size()\n",
    "    # Reindex the pass_counts to include all range classes & fill 0 for NaN values\n",
    "    pass_counts = pass_counts.reindex(total_counts.index, fill_value=0)\n",
    "    if report_err_bar:\n",
    "        percentages, err_bar = cal_err_bar(pass_counts, total_counts)\n",
    "        err_bar = err_bar.values\n",
    "    else:\n",
    "        percentages = pass_counts / total_counts * 100\n",
    "        err_bar = None\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'range_class': total_counts.index,\n",
    "        'total_counts': total_counts.values,\n",
    "        'passed_counts': pass_counts.values,\n",
    "        'percentages': percentages.values\n",
    "    })\n",
    "\n",
    "    result_df['range_class'] = pd.Categorical(result_df['range_class'], categories=['Short', 'Medium', 'Long'], ordered=True)\n",
    "    result_df = result_df.sort_values('range_class')\n",
    "    if report_err_bar:\n",
    "        result_df['percentages'] = result_df.apply(lambda row: f\"{row['percentages']*100:.1f} ± {err_bar[row.name]*100:.1f}\", axis=1)\n",
    "    else:\n",
    "        result_df['percentages'] = result_df['percentages'].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "    return percentages, result_df\n",
    "\n",
    "# percentages, result_df = classify_and_calculate(combined_df['gpt-4-turbo_with_afterlines'], 30, 100)\n",
    "# result_df\n",
    "\n",
    "def process_all_dfs(combined_df, short_range, medium_range):\n",
    "    all_results = []\n",
    "    for key in combined_df:\n",
    "        percentages, result_df = classify_and_calculate(combined_df[key], short_range, medium_range)\n",
    "        result_df.insert(0, 'key', key)\n",
    "        all_results.append(result_df)\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "def split_df(final_df):\n",
    "    unique_keys = final_df['key'].unique()\n",
    "    sub_dfs = {key: final_df[final_df['key'] == key].drop(columns='key') for key in unique_keys}\n",
    "    return sub_dfs\n",
    "\n",
    "def process_and_plot(folder_path, col_to_check):\n",
    "    # Set the global variable\n",
    "    combined_df = process_folder(folder_path)\n",
    "    # Call the function\n",
    "    plot_histograms(combined_df)\n",
    "    return combined_df\n",
    "\n",
    "def process_and_display(combined_df, short_range, medium_range):\n",
    "    # Set the short and medium ranges\n",
    "    final_df = process_all_dfs(combined_df, short_range, medium_range)\n",
    "\n",
    "    sub_dfs = split_df(final_df)\n",
    "    # Sort the keys based on model_name and gen_mode\n",
    "    sorted_keys = sorted(sub_dfs.keys(), key=lambda x: (x.rsplit('_', 1)[0], x.rsplit('_', 1)[1]))\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        df = sub_dfs[key]\n",
    "        print(f\"Key: {key}\")\n",
    "        display(df)\n",
    "    return sub_dfs\n",
    "\n",
    "def bootstrap_resampling(pass_count, total_count, num_resamples=10000):\n",
    "    # Calculate model's performance\n",
    "    performance = pass_count / total_count\n",
    "\n",
    "    # Generate bootstrap resamples\n",
    "    resamples = np.random.choice([0, 1], size=(num_resamples, total_count), p=[1-performance, performance])\n",
    "\n",
    "    # Calculate pass count for each resample\n",
    "    resample_pass_counts = resamples.sum(axis=1)\n",
    "\n",
    "    # Calculate performance for each resample\n",
    "    resample_performances = resample_pass_counts / total_count\n",
    "\n",
    "    # Calculate average and 1.96 standard deviations of resample performances\n",
    "    avg_performance = resample_performances.mean()\n",
    "    std_dev_performance = resample_performances.std()\n",
    "\n",
    "    return avg_performance, 1.96 * std_dev_performance\n",
    "\n",
    "def cal_err_bar(pass_counts, total_counts, num_resamples=10000):\n",
    "    percentages = []\n",
    "    err_bars = []\n",
    "    for pass_count, total_count in zip(pass_counts, total_counts):\n",
    "        # Use bootstrap resampling to calculate average performance and error bar\n",
    "        percentage, err_bar = bootstrap_resampling(pass_count, total_count, num_resamples)\n",
    "        percentages.append(percentage)\n",
    "        err_bars.append(err_bar)\n",
    "\n",
    "    return pd.Series(percentages, index=total_counts.index), pd.Series(err_bars, index=total_counts.index)\n",
    "\n",
    "def save_df_dict_to_csv_with_keys(df_dict, output_file_name):\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for key, df in df_dict.items():\n",
    "        key_row = pd.DataFrame({col: [key] if col == list(df.columns)[0] else [pd.NA] for col in df.columns})\n",
    "        final_df = pd.concat([final_df, key_row, df], ignore_index=True)\n",
    "\n",
    "    final_df.to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Python Completion #####################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid path: ../Analysis_Results/storage_server/Python_all_res/Completion/4th_post_process_reason_update/Update_labels. Path must be a directory containing CSV files or a CSV file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m4/5zljn3wx68n5g4glm_jx9p4r0000gn/T/ipykernel_17159/469330483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0manalyze_language_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Completion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0manalyze_language_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Infilling\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0manalyze_language_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Completion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m4/5zljn3wx68n5g4glm_jx9p4r0000gn/T/ipykernel_17159/469330483.py\u001b[0m in \u001b[0;36manalyze_language_results\u001b[0;34m(language, analysis_type)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmedium_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'medium_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdict_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_and_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedium_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msave_df_dict_to_csv_with_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m4/5zljn3wx68n5g4glm_jx9p4r0000gn/T/ipykernel_17159/1955227013.py\u001b[0m in \u001b[0;36mprocess_and_plot\u001b[0;34m(folder_path, col_to_check)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;31m# Set the global variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mplot_histograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m4/5zljn3wx68n5g4glm_jx9p4r0000gn/T/ipykernel_17159/1955227013.py\u001b[0m in \u001b[0;36mprocess_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# If the path is not a directory and not a CSV file, raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid path: {folder_path}. Path must be a directory containing CSV files or a CSV file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mprocessed_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgroup_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid path: ../Analysis_Results/storage_server/Python_all_res/Completion/4th_post_process_reason_update/Update_labels. Path must be a directory containing CSV files or a CSV file."
     ]
    }
   ],
   "source": [
    "def analyze_language_results(language, analysis_type):\n",
    "    print(f\"##################### {language} {analysis_type} #####################\")\n",
    "    settings = {\n",
    "        \"Python\": {\n",
    "            \"Completion\": {\"folder_path\": \"../Analysis_Results/storage_server/Python_all_res/Completion/4th_post_process_reason_update/Update_labels\", \"save_file_name\": \"Python_Completion_grouped_by_ref_dist.csv\", \"short_range\": 10, \"medium_range\": 30},\n",
    "            \"Infilling\": {\"folder_path\": \"../Analysis_Results/storage_server/Python_all_res/Infilling/4th_post_process_reason_update/Update_labels\", \"save_file_name\": \"Python_Infilling_grouped_by_ref_dist.csv\", \"short_range\": 10, \"medium_range\": 30}\n",
    "        },\n",
    "        \"Java\": {\n",
    "            \"Completion\": {\"folder_path\": \"../Analysis_Results/storage_server/Java_all_res/Completion/4th_post_process_reason_update/Update_labels\", \"save_file_name\": \"Java_Completion_grouped_by_ref_dist.csv\", \"short_range\": 30, \"medium_range\": 100},\n",
    "            \"Infilling\": {\"folder_path\": \"../Analysis_Results/storage_server/Java_all_res/Infilling/4th_post_process_reason_update/Update_labels\", \"save_file_name\": \"Java_Infilling_grouped_by_ref_dist.csv\", \"short_range\": 30, \"medium_range\": 100}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    setting = settings[language][analysis_type]\n",
    "    folder_path = setting['folder_path']\n",
    "    save_file_name = setting['save_file_name']\n",
    "    short_range = setting['short_range']\n",
    "    medium_range = setting['medium_range']\n",
    "\n",
    "    combined_df = process_and_plot(folder_path, col_to_check)\n",
    "    dict_dfs = process_and_display(combined_df, short_range, medium_range)\n",
    "    save_df_dict_to_csv_with_keys(dict_dfs, save_file_name)\n",
    "\n",
    "# Set the global variable\n",
    "# col_to_check = 'gen_code_pass_ratio'\n",
    "col_to_check = 'post_process_pass_ratio'\n",
    "\n",
    "# Example usage\n",
    "analyze_language_results(\"Python\", \"Completion\")\n",
    "analyze_language_results(\"Python\", \"Infilling\")\n",
    "analyze_language_results(\"Java\", \"Completion\")\n",
    "analyze_language_results(\"Java\", \"Infilling\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
